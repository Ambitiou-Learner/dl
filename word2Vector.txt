from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import nltk

nltk.download('punkt')

# Example: your own sentences
text = """
France is a country in Europe.
Paris is the capital of France.
Germany shares a border with France.
"""

# Tokenize into sentences and then words
sentences = [word_tokenize(sent.lower()) for sent in text.split('\n') if sent]

# Train a Word2Vec model
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)

# Example: Get vector for "france"
print(model.wv['france'])

# Find similar words
print(model.wv.most_similar('france', topn=3))
