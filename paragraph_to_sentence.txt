import gensim.downloader as api
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import spacy
print("Loading SpaCy model...")
nlp = spacy.load('en_core_web_sm')
print("SpaCy model loaded.")
print("Loading FastText model...")
model = api.load('fasttext-wiki-news-subwords-300') # ~1GB
print("FastText model loaded.")
paragraph = """
Artificial Intelligence is changing the world. It is being used in healthcare,␣
↪education, and finance.
AI systems can diagnose diseases and automate tedious tasks. The future of AI␣
↪is both exciting and uncertain.
"""
doc = nlp(paragraph)
sentences = [sent.text.strip() for sent in doc.sents]
def sentence_to_vec(sentence, model):
words = [token.text.lower() for token in nlp(sentence) if token.is_alpha]
vectors = [model[word] for word in words if word in model]
return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)
sentence_vectors = [sentence_to_vec(sent, model) for sent in sentences]
paragraph_vector = np.mean(sentence_vectors, axis=0)
similarities = [cosine_similarity([vec], [paragraph_vector])[0][0] for vec in␣
↪sentence_vectors]
most_rep_index = np.argmax(similarities)
most_rep_sentence = sentences[most_rep_index]
print("\nSentences and their similarity to the paragraph:")
for i, sent in enumerate(sentences):
print(f"{i+1}. {sent} (Similarity: {similarities[i]:.4f})")
print(f"\n Most representative sentence:\n\"{most_rep_sentence}\""
